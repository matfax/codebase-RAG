#!/bin/bash

# Local GitHub Workflows Testing Script
# This script mimics the GitHub Actions workflows for local testing

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Function to print colored output
print_status() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

print_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Function to check if command exists
command_exists() {
    command -v "$1" >/dev/null 2>&1
}

# Check prerequisites
check_prerequisites() {
    print_status "Checking prerequisites..."

    local missing_deps=()

    if ! command_exists uv; then
        missing_deps+=("uv")
    fi

    if ! command_exists docker; then
        missing_deps+=("docker")
    fi

    if ! command_exists docker-compose; then
        missing_deps+=("docker-compose")
    fi

    if [ ${#missing_deps[@]} -ne 0 ]; then
        print_error "Missing dependencies: ${missing_deps[*]}"
        print_status "Install missing dependencies and try again"
        exit 1
    fi

    print_success "All prerequisites met"
}

# Test main workflow (tests, linting, etc.)
test_main_workflow() {
    print_status "Testing main workflow components..."

    # Activate virtual environment
    print_status "Setting up Python environment..."
    if [ ! -d ".venv" ]; then
        uv venv
    fi
    source .venv/bin/activate

    # Install dependencies
    print_status "Installing dependencies..."
    uv pip install -e .
    uv pip install pytest pytest-cov pytest-asyncio ruff black mypy

    # Code quality checks
    print_status "Running code quality checks..."
    echo "  - Running ruff..."
    ruff check src/ tests/ --fix || print_warning "Ruff found issues"

    echo "  - Running black..."
    black --check src/ tests/ || print_warning "Black formatting issues"

    echo "  - Running mypy..."
    mypy src/ --ignore-missing-imports || print_warning "MyPy type checking issues"

    # Start services for testing
    print_status "Starting test services..."

    # Check if services are needed
    if docker ps --format "table {{.Names}}" | grep -q "test-redis\|test-qdrant"; then
        print_status "Test services already running"
    else
        print_status "Starting Redis and Qdrant for testing..."
        docker run -d --name test-redis -p 6379:6379 redis:7.2 || print_warning "Redis already running or failed to start"
        docker run -d --name test-qdrant -p 6333:6333 -e QDRANT__SERVICE__HTTP_PORT=6333 qdrant/qdrant:v1.9.0 || print_warning "Qdrant already running or failed to start"

        # Wait for services
        print_status "Waiting for services to be ready..."
        timeout 60 bash -c 'until printf "" 2>>/dev/null >>/dev/tcp/localhost/6379; do sleep 1; done' || print_error "Redis not ready"
        timeout 60 bash -c 'until curl -f http://localhost:6333/health >/dev/null 2>&1; do sleep 1; done' || print_error "Qdrant not ready"
    fi

    # Run tests
    print_status "Running tests with coverage..."
    export REDIS_URL="redis://localhost:6379"
    export QDRANT_URL="http://localhost:6333"
    export PYTHONPATH="$PWD"

    python -m pytest tests/ -v --cov=src --cov-report=xml --cov-report=html --cov-fail-under=90 || {
        print_error "Tests failed"
        return 1
    }

    # Performance regression test
    print_status "Running performance regression test..."
    python -c "
import time
import sys
sys.path.append('.')

start_time = time.time()
try:
    from src.services.cache_service import get_cache_service
    print('Cache service import: OK')
except Exception as e:
    print(f'Cache service import failed: {e}')

try:
    from src.services.indexing_service import IndexingService
    indexing = IndexingService()
    print('IndexingService creation: OK')
except Exception as e:
    print(f'IndexingService creation failed: {e}')

end_time = time.time()
print(f'Basic services initialization time: {end_time - start_time:.2f}s')

if end_time - start_time > 5.0:
    print('WARNING: Services taking too long to initialize')
    sys.exit(1)
" || print_warning "Performance test failed"

    print_success "Main workflow tests completed"
}

# Test Docker workflow
test_docker_workflow() {
    print_status "Testing Docker workflow components..."

    # Create test Dockerfile (simpler version)
    print_status "Creating test Dockerfile..."
    cat > Dockerfile.test << 'EOF'
FROM python:3.11-slim

WORKDIR /app

# Install uv
COPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv

# Copy dependency files
COPY pyproject.toml ./

# Install dependencies (without lock file for testing)
RUN uv venv && uv pip install -e .

# Copy source
COPY src/ src/

# Set environment
ENV PATH="/app/.venv/bin:$PATH"
ENV PYTHONPATH="/app/src"

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=30s --retries=3 \
  CMD python -c "import src; print('Health check passed')" || exit 1

CMD ["python", "-c", "import src; print('Docker test successful')"]
EOF

    # Build test image
    print_status "Building Docker test image..."
    docker build -f Dockerfile.test -t agentic-rag:test . || {
        print_error "Docker build failed"
        return 1
    }

    # Test image
    print_status "Testing Docker image..."
    docker run --rm agentic-rag:test || {
        print_error "Docker image test failed"
        return 1
    }

    # Test with services
    print_status "Testing Docker image with services..."
    docker run --rm \
        --add-host=host.docker.internal:host-gateway \
        -e REDIS_URL="redis://host.docker.internal:6379" \
        -e QDRANT_URL="http://host.docker.internal:6333" \
        agentic-rag:test python -c "
import os
print(f'Redis URL: {os.getenv(\"REDIS_URL\")}')
print(f'Qdrant URL: {os.getenv(\"QDRANT_URL\")}')
import src
print('âœ… Docker integration test passed')
" || print_warning "Docker integration test failed"

    # Cleanup
    rm -f Dockerfile.test
    docker rmi agentic-rag:test >/dev/null 2>&1 || true

    print_success "Docker workflow tests completed"
}

# Test Performance workflow
test_performance_workflow() {
    print_status "Testing performance workflow components..."

    # Create test performance script
    print_status "Running performance benchmarks..."

    source .venv/bin/activate
    export REDIS_URL="redis://localhost:6379"
    export QDRANT_URL="http://localhost:6333"
    export PYTHONPATH="$PWD"

    # Simple indexing benchmark
    python -c "
import time
import tempfile
from pathlib import Path

def simple_indexing_benchmark():
    print('Running simple indexing benchmark...')

    # Create test files
    test_dir = Path(tempfile.mkdtemp())
    for i in range(10):
        (test_dir / f'test_{i}.py').write_text(f'# Test file {i}\nprint(\"hello world\")')

    start_time = time.time()

    # Simulate indexing
    file_count = len(list(test_dir.glob('*.py')))
    time.sleep(0.1)  # Simulate processing

    end_time = time.time()
    duration = end_time - start_time

    print(f'Processed {file_count} files in {duration:.2f}s')
    print(f'Rate: {file_count/duration:.1f} files/sec')

    # Cleanup
    import shutil
    shutil.rmtree(test_dir)

    return duration < 5.0

def simple_search_benchmark():
    print('Running simple search benchmark...')

    queries = ['function', 'class', 'import']
    start_time = time.time()

    for query in queries:
        # Simulate search
        time.sleep(0.05)  # 50ms per query

    end_time = time.time()
    duration = end_time - start_time

    print(f'Processed {len(queries)} queries in {duration:.2f}s')
    print(f'Rate: {len(queries)/duration:.1f} queries/sec')

    return duration < 2.0

# Run benchmarks
indexing_ok = simple_indexing_benchmark()
search_ok = simple_search_benchmark()

if not indexing_ok or not search_ok:
    print('Performance benchmarks failed')
    exit(1)

print('âœ… Performance benchmarks passed')
" || {
        print_error "Performance tests failed"
        return 1
    }

    print_success "Performance workflow tests completed"
}

# Security checks
test_security_workflow() {
    print_status "Testing security workflow components..."

    # Install security tools if available
    if command_exists bandit; then
        print_status "Running Bandit security scan..."
        bandit -r src/ -f json -o bandit-report.json || print_warning "Bandit found issues"
    else
        print_warning "Bandit not installed, skipping security scan"
    fi

    if command_exists safety; then
        print_status "Running Safety check..."
        safety check --json --output safety-report.json || print_warning "Safety found issues"
    else
        print_warning "Safety not installed, skipping dependency check"
    fi

    print_success "Security workflow tests completed"
}

# Cleanup function
cleanup() {
    print_status "Cleaning up test resources..."

    # Stop test containers
    docker stop test-redis test-qdrant >/dev/null 2>&1 || true
    docker rm test-redis test-qdrant >/dev/null 2>&1 || true

    # Remove test files
    rm -f bandit-report.json safety-report.json coverage.xml
    rm -rf htmlcov/ .pytest_cache/

    print_success "Cleanup completed"
}

# Main function
main() {
    echo -e "${BLUE}ðŸ§ª Local GitHub Workflows Testing${NC}"
    echo "=================================="

    # Set trap for cleanup
    trap cleanup EXIT

    # Check arguments
    case "${1:-all}" in
        "main")
            check_prerequisites
            test_main_workflow
            ;;
        "docker")
            check_prerequisites
            test_docker_workflow
            ;;
        "performance")
            check_prerequisites
            test_performance_workflow
            ;;
        "security")
            check_prerequisites
            test_security_workflow
            ;;
        "all")
            check_prerequisites
            test_main_workflow
            test_docker_workflow
            test_performance_workflow
            test_security_workflow
            ;;
        "help"|"-h"|"--help")
            echo "Usage: $0 [main|docker|performance|security|all|help]"
            echo ""
            echo "Commands:"
            echo "  main        - Test main workflow (tests, linting, coverage)"
            echo "  docker      - Test Docker build and functionality"
            echo "  performance - Test performance benchmarks"
            echo "  security    - Test security scans"
            echo "  all         - Run all tests (default)"
            echo "  help        - Show this help message"
            exit 0
            ;;
        *)
            print_error "Unknown command: $1"
            echo "Use '$0 help' for usage information"
            exit 1
            ;;
    esac

    print_success "All selected tests completed successfully! ðŸŽ‰"
}

# Run main function with all arguments
main "$@"
